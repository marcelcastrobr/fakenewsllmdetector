{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct with Bedrock: using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: sengledo+bedrock-benelux-Admin\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "# OpenAI or Bedrock\n",
    "#llm = OpenAI(temperature=0)\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from myutils import bedrock, print_ww\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_PROFILE'] = 'sengledo+bedrock-benelux-Admin'\n",
    "boto3_bedrock = bedrock.get_bedrock_client(os.environ.get('BEDROCK_ASSUME_ROLE', None))\n",
    "\n",
    "\n",
    "#model_kwargs={'max_tokens_to_sample':200}\n",
    "model_kwargs={\n",
    "        'temperature': 0.0, 'maxTokenCount': 4096\n",
    "    }\n",
    "\n",
    "llm = Bedrock(model_id=\"amazon.titan-tg1-large\", client=boto3_bedrock)\n",
    "#llm = Bedrock(model_id=\"anthropic.claude-v1\", client=boto3_bedrock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
    "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html?highlight=agenttype#langchain.agents.agent_types.AgentType\n",
    "\n",
    "ReACT using wikipedia and serapi tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools = load_tools([\"serpapi\", \"wikipedia\", \"llm-math\"], llm=llm)\n",
    "tools = load_tools([\"wikipedia\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         handle_parsing_errors=True)\n",
    "\n",
    "#agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")\n",
    "q = \"The first woman to receive a Ph.D. in computer science was Dr. Barbara Liskov, who earned her degree from Stanford University in 1968.\"\n",
    "#q=\"Who is the first woman to receive a Ph.D. in computer science?\"\n",
    "#q = \"Jair Bolsonaro was the first man to walk over the moon in 1967.\"\n",
    "answer = agent.run(q)\n",
    "print(q)\n",
    "#template = \"\"\"Is this observation correct: {observation}\"\"\"\n",
    "#answer = agent.run(template.format(observation=q))\n",
    "#print(template.format(observation=q))\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents + CoT + Bedrock"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentType.CONVERSATIONAL_REACT_DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://amzn-aws.slack.com/archives/C05EBNPFEPN/p1692273353672639?thread_ts=1692270494.852529&cid=C05EBNPFEPN\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "tools = load_tools([\"serpapi\",\"wikipedia\"], llm=llm)\n",
    "\n",
    "agent_chain = initialize_agent(tools, llm,\n",
    "                               agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "                               verbose=True,\n",
    "                               memory=memory\n",
    "                               ,\n",
    "                               agent_kwargs={'prefix': '''\n",
    "Assistant is a large language model trained by Amazon.\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "TOOLS:\n",
    "------\n",
    "Assistant has access to the following tools:'''})\n",
    "#agent_chain.run(input=\"hi, i am Joseba\")\n",
    "#agent_chain.run(input=\"What's the first letter in my name?\")\n",
    "agent_chain.run(input= \"Is this statement correct? The first woman to receive a Ph.D. in computer science was Dr. Barbara Liskov, who earned her degree from Stanford University in 1968.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentType.ZERO_SHOT_REACT_DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action::  I need to verify if the first woman to receive a Ph.D. in computer science was Dr. Barbara Liskov\n",
      "Action: I will use Wikipedia to search for \"first woman to receive a Ph.D. in computer science\"\n",
      "Action Input: first woman to receive a Ph.D. in computer science\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: FALSE\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: FALSE\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FALSE'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://amzn-aws.slack.com/archives/C05EBNPFEPN/p1692273353672639?thread_ts=1692270494.852529&cid=C05EBNPFEPN\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "tools = load_tools([\"serpapi\",\"wikipedia\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm,\n",
    "                               agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                               verbose=True,\n",
    "                               handle_parsing_errors=True,\n",
    "                               )\n",
    "input = \"\"\"Here is a statement:\n",
    "    {statement}\n",
    "    Is this statement is correct? You can use tools to find information if needed.\n",
    "    The final response is FALSE if the statement is FALSE. Otherwise, TRUE.\"\"\"\n",
    "\n",
    "q= \"The first woman to receive a Ph.D. in computer science was Dr. Barbara Liskov, who earned her degree from Stanford University in 1968.\"\n",
    "agent.run(input.format(statement=q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_NOTEBOOK_NAME='ReAct'\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"fakenewsevaluation\", \n",
    "    job_type=\"generation\")\n",
    "\n",
    "# Define W&B Table to store generations\n",
    "columns = [\"model\", \"question\", \"answer\", \"prompt\"]\n",
    "table = wandb.Table(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI\n",
    "\n",
    "#llm = OpenAI(temperature=0)\n",
    "#agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "#agent.run(\"How many people live in canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
